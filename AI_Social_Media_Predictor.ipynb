{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c31eef0b-210f-4a3d-b19f-96f30de7f568",
   "metadata": {},
   "source": [
    "# AI-Based Social Media Post & Reel Performance Predictor\n",
    "## Using Multimodal Machine Learning (Text + Image + Video + Audio)\n",
    "\n",
    "This project predicts Instagram post/reel performance (High/Medium/Low) using multimodal features. Adapted to your dataset: image_path, caption, hashtags, likes.\n",
    "\n",
    "**Platform**: Instagram\n",
    "**Dashboard**: Streamlit (code provided at the end)\n",
    "**Dataset**: `Projects/Untitled Folder/instagram_dataset.csv` (columns: image_path, caption, hashtags, likes)\n",
    "\n",
    "Install dependencies: `pip install torch torchvision transformers scikit-learn pandas numpy opencv-python librosa streamlit pillow requests`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c03ccf4-b803-4717-aef3-3a106c01d551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sid\\anaconda3\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "385b2f6a-3039-47d2-a506-d0dd9a0ed738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in c:\\users\\sid\\anaconda3\\lib\\site-packages (11.1.0)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\sid\\anaconda3\\lib\\site-packages (4.12.0.88)\n",
      "Requirement already satisfied: numpy<2.3.0,>=2 in c:\\users\\sid\\anaconda3\\lib\\site-packages (from opencv-python) (2.1.3)\n",
      "Requirement already satisfied: librosa in c:\\users\\sid\\anaconda3\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in c:\\users\\sid\\anaconda3\\lib\\site-packages (from librosa) (3.1.0)\n",
      "Requirement already satisfied: numba>=0.51.0 in c:\\users\\sid\\anaconda3\\lib\\site-packages (from librosa) (0.61.0)\n",
      "Requirement already satisfied: numpy>=1.22.3 in c:\\users\\sid\\anaconda3\\lib\\site-packages (from librosa) (2.1.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\sid\\anaconda3\\lib\\site-packages (from librosa) (1.15.3)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in c:\\users\\sid\\anaconda3\\lib\\site-packages (from librosa) (1.6.1)\n",
      "Requirement already satisfied: joblib>=1.0 in c:\\users\\sid\\anaconda3\\lib\\site-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\sid\\anaconda3\\lib\\site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in c:\\users\\sid\\anaconda3\\lib\\site-packages (from librosa) (0.13.1)\n",
      "Requirement already satisfied: pooch>=1.1 in c:\\users\\sid\\anaconda3\\lib\\site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in c:\\users\\sid\\anaconda3\\lib\\site-packages (from librosa) (1.0.0)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in c:\\users\\sid\\anaconda3\\lib\\site-packages (from librosa) (4.12.2)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in c:\\users\\sid\\anaconda3\\lib\\site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\users\\sid\\anaconda3\\lib\\site-packages (from librosa) (1.0.3)\n",
      "Requirement already satisfied: standard-aifc in c:\\users\\sid\\anaconda3\\lib\\site-packages (from librosa) (3.13.0)\n",
      "Requirement already satisfied: standard-sunau in c:\\users\\sid\\anaconda3\\lib\\site-packages (from librosa) (3.13.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\sid\\anaconda3\\lib\\site-packages (from lazy_loader>=0.1->librosa) (24.2)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in c:\\users\\sid\\anaconda3\\lib\\site-packages (from numba>=0.51.0->librosa) (0.44.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\sid\\anaconda3\\lib\\site-packages (from pooch>=1.1->librosa) (4.3.7)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\sid\\anaconda3\\lib\\site-packages (from pooch>=1.1->librosa) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\sid\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sid\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sid\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sid\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.4.26)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\sid\\anaconda3\\lib\\site-packages (from scikit-learn>=1.1.0->librosa) (3.5.0)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\sid\\anaconda3\\lib\\site-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\sid\\anaconda3\\lib\\site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
      "Requirement already satisfied: standard-chunk in c:\\users\\sid\\anaconda3\\lib\\site-packages (from standard-aifc->librosa) (3.13.0)\n",
      "Requirement already satisfied: audioop-lts in c:\\users\\sid\\anaconda3\\lib\\site-packages (from standard-aifc->librosa) (0.2.2)\n",
      "Requirement already satisfied: torch in c:\\users\\sid\\anaconda3\\lib\\site-packages (2.9.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\sid\\anaconda3\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\sid\\anaconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\sid\\anaconda3\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\sid\\anaconda3\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sid\\anaconda3\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\sid\\anaconda3\\lib\\site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sid\\anaconda3\\lib\\site-packages (from torch) (72.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\sid\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sid\\anaconda3\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: transformers in c:\\users\\sid\\anaconda3\\lib\\site-packages (4.57.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\sid\\anaconda3\\lib\\site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\sid\\anaconda3\\lib\\site-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\sid\\anaconda3\\lib\\site-packages (from transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sid\\anaconda3\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sid\\anaconda3\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\sid\\anaconda3\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\sid\\anaconda3\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\sid\\anaconda3\\lib\\site-packages (from transformers) (0.22.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\sid\\anaconda3\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\sid\\anaconda3\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\sid\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sid\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\sid\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\sid\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sid\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sid\\anaconda3\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sid\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\sid\\anaconda3\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\sid\\anaconda3\\lib\\site-packages (from scikit-learn) (2.1.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\sid\\anaconda3\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\sid\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\sid\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pillow\n",
    "!pip install opencv-python\n",
    "!pip install librosa\n",
    "!pip install torch\n",
    "!pip install transformers\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b40101e9-c35c-4e69-acb7-1cbfd2452201",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import librosa\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, CLIPProcessor, CLIPModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2235d6f6-44c5-43e4-93b9-3dfbe0a187c5",
   "metadata": {},
   "source": [
    "## Step 1: Load and Explore Dataset\n",
    "Load your dataset with columns: image_path, caption, hashtags, likes. Supplement with open datasets if needed.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff6abd5d-cc3c-46f2-8b71-e146f2fb430f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                               image_path  \\\n",
      "0   1  https://picsum.photos/seed/pic1/600/600   \n",
      "1   2  https://picsum.photos/seed/pic2/600/600   \n",
      "2   3  https://picsum.photos/seed/pic3/600/600   \n",
      "3   4  https://picsum.photos/seed/pic4/600/600   \n",
      "4   5  https://picsum.photos/seed/pic5/600/600   \n",
      "\n",
      "                             caption                                hashtags  \\\n",
      "0   Exploring AI-powered creativity!        #AI #MachineLearning #Innovation   \n",
      "1         Design meets intelligence.                  #Design #AIArt #Future   \n",
      "2   Unlocking insights through data.           #DataScience #Analytics #Tech   \n",
      "3   Building smarter social content.  #ContentStrategy #SocialMedia #AItools   \n",
      "4  Creativity boosted by algorithms.       #CreativeAI #DeepLearning #Trends   \n",
      "\n",
      "   likes  \n",
      "0    120  \n",
      "1     95  \n",
      "2    150  \n",
      "3    180  \n",
      "4    210  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   id          5 non-null      int64 \n",
      " 1   image_path  5 non-null      object\n",
      " 2   caption     5 non-null      object\n",
      " 3   hashtags    5 non-null      object\n",
      " 4   likes       5 non-null      int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 332.0+ bytes\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Load your dataset\n",
    "df = pd.read_csv('instagram_dataset.csv')\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "\n",
    "# Compute engagement score (using only likes, as per your data)\n",
    "df['engagement'] = df['likes']\n",
    "\n",
    "# Categorize performance based on likes\n",
    "thresholds = df['likes'].quantile([0.25, 0.75])\n",
    "df['performance'] = pd.cut(df['likes'], bins=[-np.inf, thresholds[0.25], thresholds[0.75], np.inf], labels=['Low', 'Medium', 'High'])\n",
    "\n",
    "# Note: No reels in this dataset; assuming all are posts (images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c41261b-9c52-459e-a33b-503cf54e234e",
   "metadata": {},
   "source": [
    "## Step 2: Feature Extraction\n",
    "Extract features from text and images (no videos/audio in your dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25e622a9-4ce4-451f-a51c-3c75b6c1208d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "# Initialize models\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32')\n",
    "clip_model = CLIPModel.from_pretrained('openai/clip-vit-base-patch32').to(device)\n",
    "\n",
    "def extract_text_features(text):\n",
    "    inputs = bert_tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "def extract_image_features(image_path):\n",
    "    try:\n",
    "        if image_path.startswith('http'):  # If URL\n",
    "            image = Image.open(requests.get(image_path, stream=True).raw).convert('RGB')\n",
    "        else:  # Local path\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "        inputs = clip_processor(images=image, return_tensors='pt').to(device)\n",
    "        with torch.no_grad():\n",
    "            features = clip_model.get_image_features(**inputs)\n",
    "        return features.squeeze().cpu().numpy()\n",
    "    except:\n",
    "        return np.zeros(512)  # CLIP image feature dim\n",
    "\n",
    "# Apply to dataset\n",
    "df['text'] = df['caption'] + ' ' + df['hashtags']\n",
    "df['text_features'] = df['text'].apply(extract_text_features)\n",
    "df['image_features'] = df['image_path'].apply(extract_image_features)\n",
    "\n",
    "# Combine features (text + image only)\n",
    "def combine_features(row):\n",
    "    text_feat = row['text_features']\n",
    "    img_feat = row['image_features']\n",
    "    return np.concatenate([text_feat, img_feat])\n",
    "\n",
    "df['combined_features'] = df.apply(combine_features, axis=1)\n",
    "X = np.array(df['combined_features'].tolist())\n",
    "y = df['performance'].astype('category').cat.codes  # Encode labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369378c2-0c10-45b6-a173-034c5499da0a",
   "metadata": {},
   "source": [
    "## Step 3: Train Model\n",
    "Train a classifier on the combined features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1789955d-20e0-43b8-aafe-9cf7b2559d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       1.0\n",
      "           1       0.00      0.00      0.00       0.0\n",
      "\n",
      "    accuracy                           0.00       1.0\n",
      "   macro avg       0.00      0.00      0.00       1.0\n",
      "weighted avg       0.00      0.00      0.00       1.0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['performance_predictor.pkl']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Random Forest\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save model\n",
    "import joblib\n",
    "joblib.dump(model, 'performance_predictor.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8c9d35-7ba3-49ea-970f-10814cce2f67",
   "metadata": {},
   "source": [
    "## Step 4: Prediction Function\n",
    "Function to predict for new content (image path, caption, hashtags)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9188eb36-dcf3-4e69-ad7c-dc28d515e696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medium\n"
     ]
    }
   ],
   "source": [
    "def predict_performance(caption, hashtags, image_path=None):\n",
    "    text = caption + ' ' + hashtags\n",
    "    text_feat = extract_text_features(text)\n",
    "    img_feat = extract_image_features(image_path) if image_path else np.zeros(512)\n",
    "    features = np.concatenate([text_feat, img_feat]).reshape(1, -1)\n",
    "    pred = model.predict(features)[0]\n",
    "    labels = ['Low', 'Medium', 'High']\n",
    "    return labels[pred]\n",
    "\n",
    "# Example\n",
    "print(predict_performance('Amazing sunset!', '#sunset #photography', 'path/to/image.jpg'))  # Replace with actual path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96aa96d1-9c53-468e-815f-1372db813179",
   "metadata": {},
   "source": [
    "## Step 5: Streamlit Dashboard\n",
    "Save the code below as `app.py` and run `streamlit run app.py`.\n",
    "\n",
    "```python\n",
    "import streamlit as st\n",
    "import joblib\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel, CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "\n",
    "# Load model and processors\n",
    "model = joblib.load('performance_predictor.pkl')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32')\n",
    "clip_model = CLIPModel.from_pretrained('openai/clip-vit-base-patch32').to(device)\n",
    "\n",
    "# Feature extraction functions (copy from above)\n",
    "def extract_text_features(text):\n",
    "    inputs = bert_tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "def extract_image_features(image_path):\n",
    "    try:\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        inputs = clip_processor(images=image, return_tensors='pt').to(device)\n",
    "        with torch.no_grad():\n",
    "            features = clip_model.get_image_features(**inputs)\n",
    "        return features.squeeze().cpu().numpy()\n",
    "    except:\n",
    "        return np.zeros(512)\n",
    "\n",
    "def predict_performance(caption, hashtags, image_path=None):\n",
    "    text = caption + ' ' + hashtags\n",
    "    text_feat = extract_text_features(text)\n",
    "    img_feat = extract_image_features(image_path) if image_path else np.zeros(512)\n",
    "    features = np.concatenate([text_feat, img_feat]).reshape(1, -1)\n",
    "    pred = model.predict(features)[0]\n",
    "    labels = ['Low', 'Medium', 'High']\n",
    "    return labels[pred]\n",
    "\n",
    "st.title('Instagram Post Performance Predictor')\n",
    "st.write('Upload an image, enter caption and hashtags to predict performance.')\n",
    "\n",
    "caption = st.text_input('Caption')\n",
    "hashtags = st.text_input('Hashtags (comma-separated)')\n",
    "\n",
    "uploaded_file = st.file_uploader('Upload Image', type=['jpg', 'png'])\n",
    "if uploaded_file:\n",
    "    image = Image.open(uploaded_file)\n",
    "    st.image(image, caption='Uploaded Image')\n",
    "    if st.button('Predict'):\n",
    "        # Save temp file for processing\n",
    "        temp_path = 'temp_image.jpg'\n",
    "        image.save(temp_path)\n",
    "        pred = predict_performance(caption, hashtags, temp_path)\n",
    "        st.write(f'Predicted Performance: {pred}')\n",
    "        os.remove(temp_path)  # Clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09176096-8a8f-469a-b7a6-da383bf46dbf",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Final Notes:\n",
    "- **Running**: Paste into Jupyter cells and run. If `image_path` is a URL, it should work; if local, ensure paths are absolute.\n",
    "- **Enhancements**: If your dataset grows (e.g., add `comments`, `shares`, `video_path`), I can update further. For better accuracy, collect more data.\n",
    "- **Issues?**: If the dataset load fails or features don't extract, share error messages for debugging.\n",
    "\n",
    "Let me know how it goes or if you need more tweaks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
